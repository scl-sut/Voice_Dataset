{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install datasets pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k3DyieuRvf_n",
        "outputId": "757c34b2-8c00-4da9-f85a-a4b6d148acc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P4WolcmLQ8lD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734b0f1d-ff25-44ae-ca63-0e6fc428439d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import wave\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "6HMuYITeKtAE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetCombiner:\n",
        "    def __init__(self):\n",
        "        # self.dataset_10_seconds = load_dataset(\"pourmand1376/asr-farsi-youtube-chunked-10-seconds\",split='val')\n",
        "        # self.dataset_30_seconds = load_dataset(\"pourmand1376/asr-farsi-youtube-chunked-30-seconds\")\n",
        "        self.Shemo_path = '/content/drive/MyDrive/persian_datasets/Shemo'\n",
        "        self.Tehran_Aghrabe_path = '/content/drive/MyDrive/persian_datasets/Tehran_Aghrabe_79'\n",
        "        self.Persian_Speech_Dataset_path = '/content/drive/MyDrive/persian_datasets/persian-speech-corpus'\n",
        "\n",
        "    # Helper function to read audio file content\n",
        "    def read_audio_file(self,file_path):\n",
        "        with wave.open(file_path, 'rb') as wave_file:\n",
        "            n_frames = wave_file.getnframes()\n",
        "            frame_rate = wave_file.getframerate()\n",
        "            audio_content = wave_file.readframes(n_frames)\n",
        "        return audio_content, frame_rate\n",
        "\n",
        "    # Helper function to read transcript content\n",
        "    def read_transcript_file(self,file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            return file.read()\n",
        "\n",
        "    def load_Shemo(self):\n",
        "        audio_data = []\n",
        "        dataset_path = self.Shemo_path\n",
        "        for gender in ['female', 'male']:\n",
        "          gender_path = os.path.join(dataset_path, gender)\n",
        "          for file_name in os.listdir(gender_path):\n",
        "              if file_name.endswith('.wav'):\n",
        "                  audio_path = os.path.join(gender_path, file_name)\n",
        "                  transcript_path = os.path.join(dataset_path, 'transcript','final text', file_name.replace('.wav', '.ort'))\n",
        "\n",
        "                  if os.path.exists(transcript_path):\n",
        "                      audio_content, frame_rate = self.read_audio_file(audio_path)\n",
        "                      transcript_content = self.read_transcript_file(transcript_path)\n",
        "\n",
        "                      audio_data.append({\n",
        "                          # 'gender': gender,\n",
        "                          'audio': audio_path,\n",
        "                          'frame_rate': frame_rate,\n",
        "                          'transcript': transcript_content\n",
        "                      })\n",
        "\n",
        "        df_shemo = pd.DataFrame(audio_data)\n",
        "        return df_shemo\n",
        "\n",
        "    def hf_dataset(self, dataset):\n",
        "        audio_contents = []\n",
        "        transcript_content = []\n",
        "        frame_rates = []\n",
        "\n",
        "        # for mode in ['train', 'test', 'val']:\n",
        "        for sample in dataset:#[mode]:\n",
        "            audio_contents.append(sample['audio']['array'])\n",
        "            transcript_content.append(sample['transcription'])\n",
        "            frame_rates.append(sample['audio']['sampling_rate'])\n",
        "\n",
        "        df_hf_dataset = pd.DataFrame({\n",
        "            'audio': audio_contents,\n",
        "            'frame_rate': frame_rates,\n",
        "            'transcript': transcript_content\n",
        "        })\n",
        "\n",
        "        return df_hf_dataset\n",
        "\n",
        "    def Tehran_aghrabe(self):\n",
        "        data_path = self.Tehran_Aghrabe_path\n",
        "        audio_folder = os.path.join(data_path, 'audios_chunked_79')\n",
        "        csv_file = os.path.join(data_path, 'Tehran_Aghrabe_79.csv')\n",
        "\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        audio_contents = []\n",
        "        frame_rates = []\n",
        "        for index, row in df.iterrows():\n",
        "            audio_path = os.path.join(audio_folder, row['wav_filename'])\n",
        "            audio_content, frame_rate = self.read_audio_file(audio_path)\n",
        "            audio_contents.append(audio_path)\n",
        "            frame_rates.append(frame_rate)\n",
        "\n",
        "        df_Tehran_Aghrabe = pd.DataFrame({\n",
        "        'audio': audio_contents,\n",
        "        'frame_rate': frame_rates,\n",
        "        'transcript': df['transcript']\n",
        "        })\n",
        "\n",
        "        return df_Tehran_Aghrabe\n",
        "\n",
        "\n",
        "    # def load_Persian_Speech(self):\n",
        "    #     audio_contents = []\n",
        "    #     transcripts = []\n",
        "    #     frame_rates = []\n",
        "    #     valid_transcripts = []\n",
        "\n",
        "    #     dataset_path = self.Persian_Speech_Dataset_path\n",
        "    #     wav_folder = os.path.join(dataset_path, 'wav')\n",
        "    #     transcript_file = os.path.join(dataset_path, 'orthographic-transcript.txt')\n",
        "\n",
        "\n",
        "    #     with open(transcript_file, 'r', encoding='utf-8') as file:\n",
        "    #         for line in file:\n",
        "    #             parts = line.strip().split(' ', 1)\n",
        "    #             filename = parts[0].strip('\"')\n",
        "    #             transcript = parts[1].strip('\"')\n",
        "    #             transcripts.append((filename, transcript))\n",
        "\n",
        "    #     transcripts = pd.DataFrame(transcripts, columns=['filename', 'transcript'])\n",
        "\n",
        "    #     for index, row in transcripts.iterrows():\n",
        "    #         audio_path = os.path.join(wav_folder, row['filename'])\n",
        "    #         if os.path.exists(audio_path):\n",
        "    #             audio_content, frame_rate = self.read_audio_file(audio_path)\n",
        "    #             audio_contents.append(audio_content)\n",
        "    #             frame_rates.append(frame_rate)\n",
        "    #             valid_transcripts.append(row['transcript'])\n",
        "    #         else:\n",
        "    #             print(f\"File {audio_path} does not exist\")\n",
        "\n",
        "    #     df_persian_speech = pd.DataFrame({\n",
        "    #     'audio': audio_contents,\n",
        "    #     'frame_rate': frame_rates,\n",
        "    #     'transcript': valid_transcripts\n",
        "    #     })\n",
        "\n",
        "    #     return df_persian_speech\n",
        "\n",
        "\n",
        "    def combine_datasets(self):\n",
        "        # df_10_seconds = self.hf_dataset(self.dataset_10_seconds)\n",
        "        # df_30_seconds = self.hf_dataset(self.dataset_30_seconds)\n",
        "        df_shemo = self.load_Shemo()\n",
        "        df_Tehran_Aghrabe = self.Tehran_aghrabe()\n",
        "        # df_persian_speech = self.load_Persian_Speech()\n",
        "        print(len(df_shemo))\n",
        "        print(len(df_Tehran_Aghrabe))\n",
        "        print(len(df_persian_speech))\n",
        "\n",
        "        # Combining the datasets\n",
        "        combined_df = pd.concat([df_shemo, df_Tehran_Aghrabe, df_persian_speech], ignore_index=True)\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def get_combined_dataset(self):\n",
        "        return self.combine_datasets()\n",
        "\n"
      ],
      "metadata": {
        "id": "AEIkcGryvX9h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combiner = DatasetCombiner()\n",
        "combined_dataset = combiner.get_combined_dataset()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Kmjq7RxPR1sG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "afe4c062-97b9-4bbd-b5e5-9a6281ac98b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DatasetCombiner' object has no attribute 'load_Persian_Speech'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4a16f87d965c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcombiner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetCombiner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombined_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombiner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_combined_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-55ea5dbc721d>\u001b[0m in \u001b[0;36mget_combined_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_combined_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-55ea5dbc721d>\u001b[0m in \u001b[0;36mcombine_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mdf_shemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_Shemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mdf_Tehran_Aghrabe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTehran_aghrabe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mdf_persian_speech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_Persian_Speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_shemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Tehran_Aghrabe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DatasetCombiner' object has no attribute 'load_Persian_Speech'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(combined_dataset))\n",
        "print(combined_dataset.head())"
      ],
      "metadata": {
        "id": "JMgkuaLxqyPI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1e843095-164d-4295-e3ca-3aab2a5d5056"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'combined_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-07488a88f6dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'combined_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset.sample(n=5))"
      ],
      "metadata": {
        "id": "xCVHPl-19oIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_10_seconds = load_dataset(\"pourmand1376/asr-farsi-youtube-chunked-10-seconds\")\n",
        "# dataset_30_seconds = load_dataset(\"pourmand1376/asr-farsi-youtube-chunked-30-seconds\")"
      ],
      "metadata": {
        "id": "WziQUWdShMxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(type(dataset_10_seconds))\n",
        "# print(dataset_10_seconds['train'][1])"
      ],
      "metadata": {
        "id": "8bGjtwVFhAdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "3zC5WH2dhFFN"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}